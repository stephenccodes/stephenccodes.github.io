[
  {
    "objectID": "templates/about_page_template.html",
    "href": "templates/about_page_template.html",
    "title": "About",
    "section": "",
    "text": "some text goes here"
  },
  {
    "objectID": "templates/about_page_template.html#what-i-do-for-work",
    "href": "templates/about_page_template.html#what-i-do-for-work",
    "title": "About",
    "section": "",
    "text": "some text goes here"
  },
  {
    "objectID": "templates/about_page_template.html#what-i-do-for-fun",
    "href": "templates/about_page_template.html#what-i-do-for-fun",
    "title": "About",
    "section": "What I do for fun",
    "text": "What I do for fun\n\nsome text goes here\n\nhere is some more text\n\ncenter text here"
  },
  {
    "objectID": "blog_posts.html",
    "href": "blog_posts.html",
    "title": "Blog",
    "section": "",
    "text": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands\n\n\n\nStatistics\n\n\nAgriculture\n\n\nMEDS\n\n\nR\n\n\n\nHow the rapid rise in synthetic inputs during the 20th century affected labor needs on farms.\n\n\n\nStephen Carroll\n\n\nDec 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Analysis of the 2017 Thomas Fire\n\n\n\nQuarto\n\n\nPython\n\n\nMEDS\n\n\n\nA visualization of the extent and AQI impact\n\n\n\nStephen Carroll\n\n\nDec 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post 1\n\n\n\nQuarto\n\n\nMEDS\n\n\n\nA short catchy description of the blog post\n\n\n\nStephen Carroll\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWine & Vines\n\n\n\nQuarto\n\n\nMEDS\n\n\n\nA look into what the future of viticulture looks like\n\n\n\nStephen Carroll\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html",
    "href": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html",
    "title": "An Analysis of the 2017 Thomas Fire",
    "section": "",
    "text": "Image credit: kcbx.org\nAuthor: Stephen Carroll\nThis project repository can be found at: https://github.com/stephenccodes/thomas-fire-analysis\n\n\n\n\n\nThe intent of this notebook is to analyze two components of the 2017 Thomas Fire:\n\nuse remote sensing and fire perimeter data to create a false-color map of the fire and the surrounding area.\nuse air quality index (AQI) data to visualize the effect the fire had on air quality in surrounding communities.\n\nThis analysis examines the impact of the 2017 Thomas Fire on air quality in Santa Barbara County. The Thomas Fire burned over 280,000 acres in Ventura and Santa Barbara counties in December 2017, causing significant ecological & environmental damage and displacing communities. Wildfire smoke is a well-known health hazard, and one way to assess its impact is through air quality index measurements. The air quality index (AQI) is used to measure air pollution levels and is a scale ranging from 0 to 500. Scores between 0-50 are considered good, while those from 151-200 are unhealthy, and 301-500 are hazardous. This analysis will use AQI data to explore how the Thomas Fire influenced air quality and environmental health in Santa Barbara County.\nTo visualize these effects, the analysis will include a map that highlights the fire scar and distinguishes areas with vegetation or bare soil, providing a contrast to a true-color image. Additionally, AQI data will be represented in a line graph, illustrating how the fire impacted air quality over time. These insights will be drawn using various data analysis techniques outlined below.\n\n\n\n\nImport, clean, and manipulate a vector dataset\nClean data using pandas\nImport, clean, and manipulate an xarray dataset\nCreate a set of plots of the area in true and false-color without creating new variables\nRemove outlier values by adjusting the scale with the robust parameter.\nImport existing geo-data frame and implement it\nCreate a polished map of the false-color image, with superimposed fire perimeter\n\n\n\n\n\n\nThis data is a simplified collection of bands (red, green, blue, near-infrared, and shortwave infrared) from the Landsat Collection 2 Level-2 atmospherically corrected surface reflectance data, collected by the Landsat 8 satellite. The data was retrieved from the Microsoft Planetary Computer data catalogue and pre-processed to remove data outside land and coarsen the spatial resolution.\nDate accessed: 11/19/24\n\n\n\nThis database contains information about spatial distribution of historic wild and prescribed fires in the state of California. The data comes with a warning that it is not comprehensive, with some records lost or damaged, fire perimeters may be missing. There may also be duplicate fires and over-generalization of fire perimeters. The database is maintained by the California Department of Forestry and Fire Protection’s Fire and Resource Assessment Program.\nDate accessed: 11/19/24\n\n\n\nThese are two datasets that have daily air quality index (AQI) measurements for each county in California. We will use data for the years 2017 and 2018. The Thomas Fire was in late 2017, so it will help to understand the impact by setting a reference for ‘normal’ conditions before and after the fire occurred. The data was collected for airgov.now on behalf of the Environmental Protection Agency.\nDate accessed: 10/26/24"
  },
  {
    "objectID": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#a-visualization-of-the-extent-and-aqi-impact",
    "href": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#a-visualization-of-the-extent-and-aqi-impact",
    "title": "An Analysis of the 2017 Thomas Fire",
    "section": "",
    "text": "Image credit: kcbx.org\nAuthor: Stephen Carroll\nThis project repository can be found at: https://github.com/stephenccodes/thomas-fire-analysis"
  },
  {
    "objectID": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#about",
    "href": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#about",
    "title": "An Analysis of the 2017 Thomas Fire",
    "section": "",
    "text": "The intent of this notebook is to analyze two components of the 2017 Thomas Fire:\n\nuse remote sensing and fire perimeter data to create a false-color map of the fire and the surrounding area.\nuse air quality index (AQI) data to visualize the effect the fire had on air quality in surrounding communities.\n\nThis analysis examines the impact of the 2017 Thomas Fire on air quality in Santa Barbara County. The Thomas Fire burned over 280,000 acres in Ventura and Santa Barbara counties in December 2017, causing significant ecological & environmental damage and displacing communities. Wildfire smoke is a well-known health hazard, and one way to assess its impact is through air quality index measurements. The air quality index (AQI) is used to measure air pollution levels and is a scale ranging from 0 to 500. Scores between 0-50 are considered good, while those from 151-200 are unhealthy, and 301-500 are hazardous. This analysis will use AQI data to explore how the Thomas Fire influenced air quality and environmental health in Santa Barbara County.\nTo visualize these effects, the analysis will include a map that highlights the fire scar and distinguishes areas with vegetation or bare soil, providing a contrast to a true-color image. Additionally, AQI data will be represented in a line graph, illustrating how the fire impacted air quality over time. These insights will be drawn using various data analysis techniques outlined below.\n\n\n\n\nImport, clean, and manipulate a vector dataset\nClean data using pandas\nImport, clean, and manipulate an xarray dataset\nCreate a set of plots of the area in true and false-color without creating new variables\nRemove outlier values by adjusting the scale with the robust parameter.\nImport existing geo-data frame and implement it\nCreate a polished map of the false-color image, with superimposed fire perimeter\n\n\n\n\n\n\nThis data is a simplified collection of bands (red, green, blue, near-infrared, and shortwave infrared) from the Landsat Collection 2 Level-2 atmospherically corrected surface reflectance data, collected by the Landsat 8 satellite. The data was retrieved from the Microsoft Planetary Computer data catalogue and pre-processed to remove data outside land and coarsen the spatial resolution.\nDate accessed: 11/19/24\n\n\n\nThis database contains information about spatial distribution of historic wild and prescribed fires in the state of California. The data comes with a warning that it is not comprehensive, with some records lost or damaged, fire perimeters may be missing. There may also be duplicate fires and over-generalization of fire perimeters. The database is maintained by the California Department of Forestry and Fire Protection’s Fire and Resource Assessment Program.\nDate accessed: 11/19/24\n\n\n\nThese are two datasets that have daily air quality index (AQI) measurements for each county in California. We will use data for the years 2017 and 2018. The Thomas Fire was in late 2017, so it will help to understand the impact by setting a reference for ‘normal’ conditions before and after the fire occurred. The data was collected for airgov.now on behalf of the Environmental Protection Agency.\nDate accessed: 10/26/24"
  },
  {
    "objectID": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#true-color-image",
    "href": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#true-color-image",
    "title": "An Analysis of the 2017 Thomas Fire",
    "section": "True Color Image",
    "text": "True Color Image\n\nConstruct a file path to the Landsat data and read it in:\n\n# Import libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport rioxarray as rioxr\nimport matplotlib.pyplot as plt\n\n# Establish a file path for the landsat data\nfp = os.path.join('data/landsat8-2018-01-26-sb-simplified.nc')\n\n# Import the landsat data\nlandsat = rioxr.open_rasterio(fp)\n\n\n\nData Exploration\n\n# Find the CRS of the landsat data\nprint('CRS', landsat.rio.crs)\n\n# Find the dimensions of the landsat data\nprint('Height: ', landsat.rio.height)\nprint('Width: ', landsat.rio.width)\n\n# Find the data type of the landsat data\nprint('Data type: \\n', landsat.dtypes)\n\n# Find the geographic extent of the landsat data\nprint(landsat.rio.bounds(), '\\n')\n\nCRS EPSG:32611\nHeight:  731\nWidth:  870\nData type: \n Frozen({'red': dtype('float64'), 'green': dtype('float64'), 'blue': dtype('float64'), 'nir08': dtype('float64'), 'swir22': dtype('float64')})\n(121170.0, 3755160.0, 356070.0, 3952530.0) \n\n\n\n\n\nData Exploration Summary:\nTo begin, I viewed the landsat data to learn more about it. It has the variables ‘Red’, ‘Green’, ‘Blue’, ‘nir08’, and ‘swir12’. These variables represent energy bands, and the last two variables have unique names but are assumed to represent near-infrared and short wave infrared bands. I found the coordinate reference system(CRS) used and printed the height(731) and width(870) of the dataset. I listed the data types for each variable, as well as the geographic boundary of the data.\n\n\nDrop the band dimension of the data:\n\n# Remove the first dimension(band) and drop the associated coordinates\nlandsat = landsat.squeeze().drop_vars('band')\n\n\n\nSelect the red, green, and blue variables and convert it to an array and plot it:\n\n\nAdjust the scale used for plotting the bands to get a true color image and plot it again:\n\n# Remove the exterem values caused by cloud cover and plot it again\nlandsat[['red', 'green', 'blue']].to_array().plot.imshow(robust = True)\n\n\n\n\n\n\n\n\n\n\nA note on extreme outliers in your data:\nInitially, my plot displayed all of the values present, including extreme values caused by clouds or other interference. These extreme values affected the color scale, with most values rendering as either black or white. To remedy this, I adjusted to avoid the influence of these extreme values. When you set robust = True, the only values displayed are from the 2nd to 98th percentiles, allowing the color scale to better represent the meaningful range of values."
  },
  {
    "objectID": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#false-color-image",
    "href": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#false-color-image",
    "title": "An Analysis of the 2017 Thomas Fire",
    "section": "False color image",
    "text": "False color image\n\nPlot the short-wave infrared(swir22), near-infrared(nir08, and red variables:\nIn the true color image above, the fire scar is difficult to see. To better visualize the extent of thew fire, we can use Red, Green, and Blue hues to display other parts of the EM spectrum. In this case, we will substitue the true colors for short-wave infrared(swir22), near-infrared(nir08, and red variables.\n\n# Use SWIR, NIR, and Red wavelengths to viaulize the vegetation and burn areas better\nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust = True)"
  },
  {
    "objectID": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#false-color-composite-map",
    "href": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#false-color-composite-map",
    "title": "An Analysis of the 2017 Thomas Fire",
    "section": "False Color Composite Map",
    "text": "False Color Composite Map\n\nImport Thomas Fire perimeter and convert the CRS to match Landsat data:\n\n# Read in the Thomas fire data and convert CRS to match the landsat data\nthomas_fire = gpd.read_file('data/thomas_fire/thomas_fire.shp').to_crs(landsat.rio.crs)\n\n# Confirm the change was successful\nassert landsat.rio.crs == thomas_fire.crs\n\n\n\nCombine both elements to create a composite map:\nWe can use the border of the data and the false color image we jsut created ti make a composite map that does a great job of highlighting exactly where the 2017 Thomas Fire burned.\n\n# Define the landsat aspect ratio\nlandsat_aspect_ratio = landsat.rio.width / landsat.rio.height \n\n# Initialize the figure and set the dimensions\nfig, ax = plt.subplots(figsize = (9, 5 * landsat_aspect_ratio)) # Apply the aspect ratio\n\n# Set the background color of the entire figure \nfig.patch.set_facecolor('beige')\n\n# Remove axes for cleaner map\nax.axis('off') \n\n# Plot the false color remorte sensing data\nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust = True, ax = ax)\n\n# Plot the Thomas Fire perimeter\nthomas_fire.boundary.plot(ax = ax, \n                          color = \"red\", # Color it red for emphasis\n                          linewidth = 0.8)\n\n# Add a main title\nax.set_title(\"The 2017 Thomas Fire Scar\\n\", fontsize = 12)\n\n# Add subtitle describing the false colors\nfig.suptitle(\"False Colors with Short Wave Infrared, Near-Infrared, & Red Wavelengths\", \n             color = 'black', \n             fontsize = 10, \n             fontweight='light', \n             y=0.91)\n\n# Add a legend for the fire boundary\nax.legend(labels = ['Thomas Fire (2017) Scar'])\n\n# Add a footnote citation at the bottom of the figure\nfig.text(0.379, 0.1, # Position\n         'Data Source: CAL FIRE & USGS EROS Archive',\n         ha='center', # Horizontal alignment\n         va='center', # Vertical alignment\n         fontsize=8, \n         color='black', \n         fontstyle='italic')\nfig.text(0.395, 0.08, # Position \n         'Date Accessed: 11/19/24',\n         ha='right', # Horizontal alignment\n         va='center', # Vertical alignment\n         fontsize=8, \n         color='black', \n         fontstyle='italic')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMap Description:\nThe figure above displays the area burned during the 2017 Thomas Fire, as well as surrounding areas. The area outlined in red represents the extent of the fire perimeter. This is a false color image, with Short Wave Infrared(SWIR), Near-Infrared(NIR), and Red energy bands being visualized with Red, Green, and Blue colors, respectively.\nThe burn scar is displayed as red, because newly burned land reflects strongly in SWIR bands. The areas of the map that have vegetation are depicted by green colors, as vegetation reflect near-infrared light strongly, with healthy plants reflecting more than stressed plants. Accordingly, since the chlorophyll in plants absorbs red light, very little of the repesentative blue is displayed here."
  },
  {
    "objectID": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#data-visualization",
    "href": "blog_posts/thomas-fire-analysis/thomas_fire_analysis.html#data-visualization",
    "title": "An Analysis of the 2017 Thomas Fire",
    "section": "Data visualization:",
    "text": "Data visualization:\nNow we can plot the AQ! and 5-day average over time to see how the 2017 Thomas Fire impacted it.\n\n# Initialize a figure and axis \nfig, ax = plt.subplots(figsize = (12, 8)) # Set dimensions relatively wide to space out lines\n\n# Visualize air quality during the Thomas Fire\naqi_sb.aqi.plot(ax=ax, label = 'AQI')\naqi_sb.five_day_average.plot(ax=ax, label = \"Five day AQI average\")\n\n# Show the date for the Thomas fire\nplt.axvline(x = '2017-12-04', \n            color = 'red', \n            linestyle = 'dashed', \n            label = \"Thomas Fire\")\n\n# Labels\n# Title describing the area and event of interest\nax.set_title('2017 Thomas Fire in Santa Barbara & Ventura County', \n             color = 'black', \n             fontsize = 12, \n             fontweight='normal', \n             y=0.99)\n\n# Subtitle describing the plot\nfig.suptitle('Daily AQI and 5-day AQI averages:', \n             color = 'black', \n             fontsize = 12, \n             fontweight='heavy', \n             y=0.93)\n\nax.set_xlabel('Date')\nax.set_ylabel('AQI')\nax.legend()\n\n# Display the figure\nplt.show()\n\n\n\n\n\n\n\n\n\nReference List:\n\nU.S. Geological Survey. (n.d.). Landsat 8-9 OLI/TIRS collection 2 level-2 science products | USGS EROS Archive. U.S. Geological Survey. https://www.usgs.gov/centers/eros/science/usgs-eros-archive-landsat-archives-landsat-8-9-olitirs-collection-2-level-2 Access date: November 19, 2024.\nData.gov. (2024). California fire perimeters (ALL). Data.gov. https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436 Access date: November 19, 2024.\nAirnow.gov. (2017-2018). US Environmental Protection Agency. Air Quality Index (AQI) data from the US Environmental Protection Agency Access date: October 26, 2024.\n\n\n\nAcknowledgements\nAll materials were created by Carmen Galaz-Garcia for EDS-220: Working with Environmental Data."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About page",
    "section": "",
    "text": "Stephen holds a Masters degree in Environmental Data Science from the Bren School of Environmental Management at UC Santa Barbara. He is excited to use data science, statistics, and programming to turn messy data into clear, actionable insights that improve products, operations, and strategy.\nStephen became passionate about environmental studies as an undergraduate at UC Santa Cruz, where he interned with the UC Natural Reserve System, working to restore coastal prairie habitat. After graduating with an Anthropology degree in 2014, Stephen worked in agriculture, first, at an agri-tech startup with a mission to transform local food systems through vertical hydroponic farms, and later, at an organic farm, where he served in a leadership position and developed efficient approaches to workflow and logistics with the goal of reducing waste and costly inputs. Stephen also earned a certificate in microscopy and soil science, developing expertise in the microbiological processes that underpin the distinct agricultural operations in which he has taken part.\nAs a graduate student, he became an enthusiastic data scientist, problem solver, and scientific communicator while maintaining a tidy and reproducible data science practice. He especially enjoyed courses in Data Visualization, Climate Modeling, Data Management, Geospatial Analysis, Remote Sensing, and Machine Learning. In his capstone project, he helped build a data-processing pipeline that utilized machine learning and remote sensing data to estimate the emissions produced by all global fisheries, the first database of it’s kind. The results were integrated into the Seamissions Explorer an interactive web-based dashboard to support researchers and consumers in analyzing fisheries’ emissions across regions and time."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About page",
    "section": "Experience",
    "text": "Experience"
  },
  {
    "objectID": "blog_posts/stats-project/pesticides-fertilizers-labor.html",
    "href": "blog_posts/stats-project/pesticides-fertilizers-labor.html",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "",
    "text": "cnbc.com\nThe repository for this analysis can be accessed here."
  },
  {
    "objectID": "blog_posts/stats-project/pesticides-fertilizers-labor.html#introduction",
    "href": "blog_posts/stats-project/pesticides-fertilizers-labor.html#introduction",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Introduction",
    "text": "Introduction\nThe purpose of this post is to analyze the relationship between annual synthetic inputs in commercial agriculture and corresponding labor demands. It aims to examine the pesticide and fertilizer use by farms in the United States between 1960 and 2004, as well as labor needs over the same parameters, and determine how these factors affect one another."
  },
  {
    "objectID": "blog_posts/stats-project/pesticides-fertilizers-labor.html#background",
    "href": "blog_posts/stats-project/pesticides-fertilizers-labor.html#background",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Background",
    "text": "Background\nMost people don’t think much about the work that goes in to producing the food that they eat. I know I didn’t. Like many of us, I grew up disconnected from the source of my food, unaware of the hard work that brings it to our tables. It wasn’t until I became involved in agriculture that I truly understood the effort and resources required to grow the food we often take for granted. Throughout history, labor has consistently been one of the largest expenses in agriculture, both in terms of time and financial investment. From planting and cultivating to harvesting and processing crops, labor is essential to ensuring that food reaches our markets.\nI never really considered just how important it was to the economics of a farming operation until I began working for a small organic farm in Santa Cruz County, California. One of my responsibilities was to pay our vendors, and I learned that the cost of inputs—seed, fertilizer, amendments, etc.—was something that farmers are always trying to reduce. It was much harder to reduce labor. In the peak of Summer, our payroll could be nearly twice as much as it was in the Winter! That’s no small fluctuation, and it put a lot of stress on the business to operate smoothly.\nI remember my shock when I learned that our crops were not only weeded with a cultivating tractor, but by hand as well. Several times a week, our crew of more than 20 would spend hours pulling weeds in a block of lettuce or scallions. The weeds that grew too close to the crops to be uprooted by the tillage blades of the tractor were a big concern. If they didn’t cull them in time, they would outgrow the crops and the harvest would take much longer, making it cost-ineffective. During the busy Summer months when weeds grow faster and we had tomatoes and strawberries to harvest, we would occasionally lose a block of something and have to cut our losses and till the land, preparing it for a new planting.\n\n\n\nfarmworkerjustice.org\n\n\nOther times, we would be plagued with an onslaught of insects. Flea beetles that would eat holes through our lettuce leaves and blemish our radish greens. Wireworms that tunneled into our carrots and made them bitter. Fruit flies, that would feast on rotting berries and multiply by the millions if furrows weren’t quickly and meticulously cleared of any discarded fruit. One year we couldn’t keep up with the cleaning and the flies became an infestation that curtailed the entire season and spoiled the appetite of some customers. Of course, if we weren’t organic, we would have sprayed for flies and been able to continue to harvest fruit into October. It’s a bitter truth, and it caused me to reflect on conventional farms that use synthetic chemicals to control weeds and pests. No wonder organic produce is so expensive, I’d think. This realization prompted me to consider how the demand for labor in agriculture has evolved, particularly in response to the rising use of synthetic pesticides and fertilizers. I began to wonder: was labor cost minimization due to these inputs the reason large conventional farms were successful, when smaller organic operations seem to come and go?\nThis curiosity led me to explore the relationship between pesticide and fertilizer use and labor trends in agriculture. In this post, I will examine how the increasing reliance on synthetic chemicals has reshaped labor needs in the U.S.A. since 1960. Using statistical analysis, I will explore how labor costs have changed as farming practices evolved, offering insights into the broader economic and social impacts of these shifts. Through this analysis, I hope to better understand the relationship between these factors, the trajectory of labor in agriculture and the ongoing challenges to success it presents."
  },
  {
    "objectID": "blog_posts/stats-project/pesticides-fertilizers-labor.html#analysis-plan",
    "href": "blog_posts/stats-project/pesticides-fertilizers-labor.html#analysis-plan",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Analysis Plan",
    "text": "Analysis Plan\n\nMultiple Linear Regression\nTo better understand the relationship between pesticides, fertilizers, labor and time, We’ll look at historical data from 1960-2004. This period saw a dramatic rise in these synthetic inputs, so it should allow a good perspective on the relationship. Annual usage data from the US Department of Agriculture will allow us to determine the rate of this change, and annual national labor use data should give us an idea of the effect it had on farms.\nWe’ll analyze the correlation of these factors over time, which will give us an indication of their relationship. Next, I’ll implement linear regression analysis to determine the effect that pesticide and fertilizer use, and time had on labor needs. The regression model will be the following:\n\\[\n\\text{labor} = \\beta_0 + \\beta_1 \\cdot \\text{pesticide} + \\beta_2 \\cdot \\text{fertilizer} + \\beta_3 \\cdot \\text{year} + \\epsilon\n\\]\nThe response variable is ‘total_labor’, and represents all forms of labor demand in commercial agriculture operations in the country. The \\(\\beta\\)s represent the regression coefficients, each representing a predictor variable, all of which are continuous. This assumes that there is a linear relationship between ‘total_labor’ and each independent predictor variable.\nThis model will enable me to assess the extent to which changes in these factors predict the demand in labor. By examining these patterns over time we can attempt to draw conclusions about how the increasing reliance on synthetic chemicals has influenced labor dynamics in agriculture, potentially providing valuable insight into the economic impacts of different farming practices.\n\n\nGamma Regression\nThough I don’t believe my variables to be categorical, if linear regression does not yield convincing results, I will turn to gamma analysis. Having a secondary method of analysis may prove valuable if my predictor variables are insufficient to draw conclusions. Trying both models allows me to compare their performance and ensure I’m using the most appropriate approach for my analysis. Herre is an example of a model that I might use:\n\\[\\log(\\mathbb{E}[\\text{labor}]) = \\beta_0 + \\beta_1 \\cdot \\text{pesticide} + \\beta_2 \\cdot \\text{year} + \\beta_3 \\cdot (\\text{pesticide} \\times \\text{year})\\]\n\n\nHypothesis Statement\nNull Hypothesis (H₀): Pesticide and fertilizer use do not have a significant effect on labor demand.\nAlternative Hypothesis (H₁): Pesticide and fertilizer use have a significant effect on labor needs."
  },
  {
    "objectID": "blog_posts/stats-project/pesticides-fertilizers-labor.html#data",
    "href": "blog_posts/stats-project/pesticides-fertilizers-labor.html#data",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Data",
    "text": "Data\nFor this analysis, all data will be sourced from the United States Department of Agriculture (USDA) Economic Research Service.\nThe labor data comes from a report titled Agricultural productivity in the United States., and contains annual state and U.S. average statistics of agricultural inputs and outputs, including several classifications of labor indices. Indices of labor input were constructed for each State and the aggregate farm sector using the demographically cross-classified hours and compensation data. Labor hours having higher marginal productivity (wages) are given higher weights in forming the index of labor input than are hours having lower marginal productivities. Doing so explicitly adjusts indices of labor input for “quality” change in labor hour.[1] For this analysis, we will focus on the total and hired labor indices, values which are best suited to represent the labor that would be affected by changes in other inputs.\nThe pesticide and fertilizer data originates from **Fertilizer & Pesticide Use in the United States* summarizing surveyed states annual consumption of material inputs from 1960 to 2004. Indices are a relative to the rate of consumption in Alabama in 1996 based on a value of 1.\nThe data is formatted as several tables per document with headers and notes, and required reformatting, filtering, and removal of unnecessary summary tables in order to utilize further."
  },
  {
    "objectID": "blog_posts/stats-project/pesticides-fertilizers-labor.html#analysis",
    "href": "blog_posts/stats-project/pesticides-fertilizers-labor.html#analysis",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Analysis",
    "text": "Analysis\n\n\n\ncropaia.com\n\n\n\nSetup\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(here)\nlibrary(readxl)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(gridExtra)\n\n\n\n\nPesticide & Fertilizer Data Extraction\nThese two dataframes are from the same source and have identical structure. First, we have to read in the raw data for our synthetic inputs; Pesticides & Fertilizers. The data is not in a usable format, so we will have to remove anything extraneous. This allows us pivot the dataframe longer so that it’s easier to conduct analysis. We need to find the national average pesticide usage and put that value into a new column. We’ll do the same thing for our fertilizer data below.\n\n\nCode\n# Read in pesticide data\npesticide &lt;- read_excel(here(\"blog_posts/stats-project/data/pesticide_consumption_state.xls\"), skip = 5, range = \"A6:AW53\")\n\n# Remove rows that are entirely blank\npesticide_clean &lt;- pesticide[rowSums(is.na(pesticide)) &lt; ncol(pesticide), ]\n\npesticide_tidy &lt;- pesticide_clean %&gt;%\n  pivot_longer(cols = -Year, # Pivot all columns except Year\n               names_to = \"state\", # Create a new column for each state \n               values_to = \"pesticide\") %&gt;% # The index values \n  group_by(Year) %&gt;%\n  mutate(avg_pesticide = mean(pesticide)) %&gt;%  # Create a new column for mean use (National average)\n  ungroup() %&gt;%\n  distinct(Year, .keep_all = TRUE) %&gt;% # Remove duplicate rows by year\n  select(Year, avg_pesticide)  # Keep only 'Year' and 'us_avg_pesticide'\n\n\n\n\nCode\n# Read in fertilizer data\nfertilizer &lt;- read_excel(here(\"blog_posts/stats-project/data/fertilizer_consumption_state.xls\"), skip = 5, range = \"A6:AW53\")\n\n# Remove rows that are entirely blank\nfertilizer_clean &lt;- fertilizer[rowSums(is.na(fertilizer)) &lt; ncol(fertilizer), ]\n\nfertilizer_tidy &lt;- fertilizer_clean %&gt;%\n  pivot_longer(cols = -Year, # Pivot all columns except Year\n               names_to = \"state\",# Create a new column for each state\n               values_to = \"fertilizer\") %&gt;% # The index values \n  group_by(Year) %&gt;%\n  mutate(avg_fertilizer = mean(fertilizer)) %&gt;%  # Create a new column for mean use (National average)\n  ungroup() %&gt;%\n  distinct(Year, .keep_all = TRUE) %&gt;% # Remove duplicate rows by year\n  select(Year, avg_fertilizer)  # Keep only 'Year' and 'avg_fertilizer'\n\n\n\n\nLabor Data Extraction:\nNow we can process our labor data. This information is part of a diverse dataframe that has a lot of information that’s not helpful for us to look at. Let’s simplify it by filtering for observations with ‘labor’ in the title. Next we can pivot the table longer to match the format of our other data, with each attribute having its own column. Since this data is annual, we don’t need to make a new column for annual average. Now that our data is reformatted, we can filter it. We can remove the column for self-employed and family workers because these people would likely work for the farms regardless of changes in pesticide and fertilizer use.\n\n\nCode\n# Read in labor data\nlabor &lt;- read_csv(here(\"blog_posts/stats-project/data/labor_etc.csv\"))\n\n# Filter rows in the 'labor' data frame where any column contains the word \"labor\"\nfiltered_labor &lt;- labor[apply(labor, 1, function(row) any(grepl(\"labor\", row, ignore.case = TRUE))), ]\n\n# Pivot the data so that each attribute becomes its own column\nlabor_clean &lt;- filtered_labor %&gt;%\n  pivot_wider(\n    names_from = Attribute, # The column to spread into multiple columns\n    values_from = Value # The column to use as the values for each new column\n  )\n\n# Subset the labor data to keep only total labor index and hired labor index\nlabor_tidy &lt;- labor_clean[, c(\"Labor inputs: Total\", \"Labor inputs: Hired labor\", \"Year\"), drop = FALSE] %&gt;%\n  select(\"Year\", \"Labor inputs: Total\", \"Labor inputs: Hired labor\") %&gt;%\n   filter(Year &gt;= 1960) %&gt;%  # Filter for our area of interest\n  rename(\n    avg_total_labor = \"Labor inputs: Total\", # Rename the column for total labor\n    avg_hired_labor = \"Labor inputs: Hired labor\" # Rename the column for hired labor\n  )\n\n\n\n\nMerge Data Together\nWe now have three datasets that have similar format and a common column for ‘Year’. Let’s join them together to create a single daatset that we can use for the entire analysis. We’ll also add a column to indicate the amount of years elapsed from the beginning of our data, 1960.\n\n\nCode\n# Merge the pesticide_tidy, fertilizer_tidy, and labor_tidy dataframes by the 'Year' column\ncombined_tidy &lt;- left_join(pesticide_tidy, fertilizer_tidy, by = \"Year\") %&gt;%\n  left_join(labor_tidy, by = \"Year\")\n\n# Add a column for the time difference between each year and the previous year\ncombined_tidy &lt;- combined_tidy %&gt;%\n  mutate(years_elapsed = Year - min(Year))\n\n\nLet’s take a look at our tidy new dataset:\n\n\nCode\n# View the updated dataframe\nhead(combined_tidy)\n\n\n# A tibble: 6 × 6\n   Year avg_pesticide avg_fertilizer avg_total_labor avg_hired_labor\n  &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n1  1960         0.218          0.948            2.50            2.12\n2  1961         0.247          0.986            2.44            2.12\n3  1962         0.270          0.939            2.45            2.11\n4  1963         0.279          1.06             2.35            2.11\n5  1964         0.298          1.18             2.21            1.90\n6  1965         0.296          1.25             2.15            1.79\n# ℹ 1 more variable: years_elapsed &lt;dbl&gt;"
  },
  {
    "objectID": "blog_posts/stats-project/pesticides-fertilizers-labor.html#data-visualization-exploration",
    "href": "blog_posts/stats-project/pesticides-fertilizers-labor.html#data-visualization-exploration",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Data Visualization & Exploration",
    "text": "Data Visualization & Exploration\nTo get a better idea of the usage trends for our synthetic inputs, we can make line graphs with ‘Year’ on the x-axis and the index value on the y-axis. When we display them we can see clearly that both pesticide and fertilizer use has risen dramatically in the U.S. since 1960.\n\n\nCode\n# Plot the trend of pesticide use over time\npest_trend_plot &lt;- ggplot(combined_tidy, aes(x = Year, y = avg_pesticide)) +\n  geom_line(color = \"orange\", size = 1.5, alpha = 0.7) +  # Add color, size, and transparency\n  labs(\n    title = \"Average Pesticide Use Over Time\",\n    subtitle = \"Trend of U.S. Average Pesticide Use by Year (1960-2004)\",\n    x = \"Year\",\n    y = \"Average Pesticide Use (Index)\",\n    caption = \"Source: U.S. Department of Agriculture, Economic Research Service. \"  # Caption for data source\n  ) +\n  theme_minimal() +  # Clean background\n  theme(\n    text = element_text(size = 12),  # Set base font size\n    axis.title = element_text(face = \"bold\"),  # Bold axis titles\n    axis.text = element_text(color = \"black\"),  # Color for axis labels\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),  # Centered title with larger font\n    plot.subtitle = element_text(hjust = 0.5, size = 12),  # Centered subtitle\n    plot.caption = element_text(hjust = 0, size = 10, face = \"italic\")  # Caption style\n  )\n\n# Plot the trend of pesticide use over time\nfert_trend_plot &lt;- ggplot(combined_tidy, aes(x = Year, y = avg_fertilizer)) +\n  geom_line(color = \"darkgreen\", size = 1.5, alpha = 0.7) +  # Add color, size, and transparency\n  labs(\n    title = \"Average Fertilizer Use Over Time\",\n    subtitle = \" Trend of U.S. Average Fertilizer Use (1960-2004)\",\n    x = \"Year\",\n    y = \"Average Fertilizer Use (Index)\",\n    caption = \"Source: U.S. Department of Agriculture, Economic Research Service. \"  # Caption for data source\n  ) +\n  theme_minimal() +  # Clean background\n  theme(\n    text = element_text(size = 12),  # Set base font size\n    axis.title = element_text(face = \"bold\"),  # Bold axis titles\n    axis.text = element_text(color = \"black\"),  # Color for axis labels\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),  # Centered title with larger font\n    plot.subtitle = element_text(hjust = 0.5, size = 12),  # Centered subtitle\n    plot.caption = element_text(hjust = 0, size = 10, face = \"italic\")  # Caption style\n  )\n\n# Display graphs next to each other\nfert_trend_plot + pest_trend_plot + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\nNow, let’s determine the trend for our labor variables. We’ll plot them together on the same graph because their scale is the same. When we do we can see that both hired and total labor has decreased precipitously since 1960.\n\n\nCode\n# Plot the trend of labor needs over time\nggplot(combined_tidy, aes(x = Year)) +\n  geom_line(aes(y = avg_total_labor, color = \"Total Labor\"), size = 1.5) +  \n  geom_line(aes(y = avg_hired_labor, color = \"Hired Labor\"), size = 1.5) + \n  scale_color_manual(values = c(\"Total Labor\" = \"blue\", \"Hired Labor\" = \"red\")) +  \n  labs(\n    title = \"Labor Inputs: Total vs. Hired Labor Over Time\",\n    subtitle = \"Trend of U.S. Average Total Labor and Hired Labor Input Indices (1960-2004)\",\n    x = \"Year\",\n    y = \"Labor Inputs (Index)\",\n    color = \"Labor Type\"  # Legend title\n  ) +\n  theme_minimal() +  # Clean theme\n  theme(\n    text = element_text(size = 12),  # Set base font size\n    axis.title = element_text(face = \"bold\"),  # Bold axis titles\n    axis.text = element_text(color = \"black\"),  # Axis labels color\n    legend.title = element_text(face = \"bold\"),  # Bold legend title\n    legend.position = \"top\",  # Position the legend at the top\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),  # Centered plot title with larger font size\n    plot.subtitle = element_text(hjust = 0.5, size = 12)  # Centered subtitle\n  )\n\n\n\n\n\n\n\n\n\nTo further understand the relationship between each variable, let’s make a correlation matrix. The value for each cell represents \\(r\\), the correlation coefficient. An \\(r\\) value ranges from -1 to 1, with 0 indicating no correlation at all. A value of -1 means that the variables are a perfect negative linear relationship, white a value of 1 represents a perfect positive relationship.\n\n\nCode\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(combined_tidy[, c(\"avg_pesticide\", \"avg_fertilizer\", \"avg_total_labor\", \"avg_hired_labor\", \"years_elapsed\")])\n\n# Convert the correlation matrix into a data frame for better table formatting\ncor_df &lt;- as.data.frame(cor_matrix)\n\n# Use kable() to display the correlation matrix as a table\nkable(cor_df, digits = 2, caption = \"Correlation Matrix of Agricultural Inputs & Labor Categories (r)\") %&gt;% \n kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"center\") %&gt;%\n  column_spec(1:6, background = \"white\")\n\n\n\nCorrelation Matrix of Agricultural Inputs & Labor Categories (r)\n\n\n\navg_pesticide\navg_fertilizer\navg_total_labor\navg_hired_labor\nyears_elapsed\n\n\n\n\navg_pesticide\n1.00\n0.82\n-0.93\n-0.87\n0.98\n\n\navg_fertilizer\n0.82\n1.00\n-0.76\n-0.72\n0.83\n\n\navg_total_labor\n-0.93\n-0.76\n1.00\n0.96\n-0.95\n\n\navg_hired_labor\n-0.87\n-0.72\n0.96\n1.00\n-0.91\n\n\nyears_elapsed\n0.98\n0.83\n-0.95\n-0.91\n1.00\n\n\n\n\n\n\n\nLet’s make scatter plots as well to better visualize these relationships and confirm the results from the correlation matrix.\n\n\nCode\n#  Pesticide vs Fertilizer Use\ncor_plot1 &lt;- ggplot(combined_tidy, aes(x = avg_pesticide, y = avg_fertilizer)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"cornflowerblue\") + # Line of best fit\n  labs(title = \"Pesticide vs Fertilizer Use\", x = \"Average Pesticide\", y = \"Average Fertilizer\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Total Labor vs Hired Labor\ncor_plot2 &lt;- ggplot(combined_tidy, aes(x = avg_total_labor, y = avg_hired_labor)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"cornflowerblue\") + # Line of best fit\n  labs(title = \"Total Labor vs Hired Labor\", x = \"Average Total Labor\", y = \"Average Hired Labor\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Pesticide Use vs Total Labor\ncor_plot3 &lt;- ggplot(combined_tidy, aes(x = avg_pesticide, y = avg_total_labor)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") + # Line of best fit\n  labs(title = \"Pesticide Use vs Total Labor\", x = \"Average Pesticide\", y = \"Average Total Labor\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Fertilizer Use vs Hired Labor\ncor_plot4 &lt;- ggplot(combined_tidy, aes(x = avg_fertilizer, y = avg_hired_labor)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") + # Line of best fit\n  labs(title = \"Fertilizer Use vs Hired Labor\", x = \"Average Fertilizer\", y = \"Average Hired Labor\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Arrange all plots in a 2x2 grid\ncor_plot1 + cor_plot2 + cor_plot3 + cor_plot4 + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\nWe can see from these visualizations that pesticide and fertilizer both have a strong positive correlation with each other, and strong negative correlation with both labor variables. The labor variables have exactly the same extremely high correlation with each other, it appears that they are collinear and may not provide distinct results in other analysis. Because they are essentially redundant, they probably won’t be effective in our model."
  },
  {
    "objectID": "blog_posts/stats-project/pesticides-fertilizers-labor.html#linear-regression-analysis",
    "href": "blog_posts/stats-project/pesticides-fertilizers-labor.html#linear-regression-analysis",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Linear Regression Analysis",
    "text": "Linear Regression Analysis\nLet’s revisit the multiple linear regression model previewed earlier and update it with our specific variable names:\n\n\\(\\text{avg\\_total\\_labor} = \\beta_0 + \\beta_1 \\cdot \\text{avg\\_pesticide} + \\beta_2 \\cdot \\text{avg\\_fertilizer} + \\beta_3 \\cdot \\text{years\\_elapsed} + \\epsilon\\)\n\nNow we can define the model in more depth given what we’ve learned about our predictor variables. The regression coefficients represent the following:\n\n\\(\\beta_0\\) = Our intercept. The ‘avg_total_labor’ value when all predictor variables are equal to zero.\n\\(\\beta_1\\) = The change in ‘avg_total_labor’ for each one unit increase in ‘avg_pesticide’.\n\\(\\beta_2\\) = The change in ‘avg_total_labor’ for each one unit increase in ‘avg_fertilizer’.\n\\(\\beta_3\\) = The change in ‘avg_total_labor’ for each one unit increase in ‘Year’.\n\n\n\nCode\n# Make a multi linear regression model predicting total labor\ntotal_labor_model &lt;- lm(avg_total_labor ~ avg_pesticide + avg_fertilizer + years_elapsed, data = combined_tidy)\n\n# Get the summary of the model to interpret the coefficients\ntotal_model_sum &lt;- summary(total_labor_model)\n\n\nWe can make a nearly identical model to predict ‘hired_labor’. Let’s do that now so we can test the results together. For this model, the regression coefficients represent the same predictors, but they are predicting ‘avg_hired_labor’ now.\n\n\nCode\n# A separate model predicting avg_total_labor using only avg_hired_labor\nhired_labor_model &lt;- lm(avg_hired_labor ~ avg_pesticide + avg_fertilizer + years_elapsed, data = combined_tidy)\n\n# Get the summary of the model to interpret the coefficients\nhired_model_sum &lt;- summary(hired_labor_model)\n\n\n\nLinear Model Evaluation\nNow that we have summaries for both of our models, we can interpret them. To visualize the coefficient better, let’s put the results in a table.\n\n\nCode\n# Extract the summaries for both models\ntotal_labor_summary &lt;- summary(total_labor_model)$coefficients\nhired_labor_summary &lt;- summary(hired_labor_model)$coefficients\n\n# Convert to data frames for easier manipulation\ntotal_labor_df &lt;- as.data.frame(total_labor_summary)\ncolnames(total_labor_df) &lt;- c(\"Estimate\", \"Std. Error\", \"t value\", \"p-value\")\ntotal_labor_df$Variable &lt;- rownames(total_labor_df)\ntotal_labor_df$Model &lt;- \"Total Labor\"\n\nhired_labor_df &lt;- as.data.frame(hired_labor_summary)\ncolnames(hired_labor_df) &lt;- c(\"Estimate\", \"Std. Error\", \"t value\", \"p-value\")\nhired_labor_df$Variable &lt;- rownames(hired_labor_df)\nhired_labor_df$Model &lt;- \"Hired Labor\"\n\n# Remove row names (index) by resetting the row names to NULL\nrownames(total_labor_df) &lt;- NULL\nrownames(hired_labor_df) &lt;- NULL\n\n# Print the tables separately\ntotal_labor_table &lt;- kable(total_labor_df[, c(\"Variable\", \"Estimate\", \"Std. Error\", \"t value\", \"p-value\")],\n                           digits = 3, \n                           caption = \"Regression Coefficients for Total Labor Model\")  %&gt;% \n kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"center\") %&gt;%\n  column_spec(1:5, background = \"white\")\n\nhired_labor_table &lt;- kable(hired_labor_df[, c(\"Variable\", \"Estimate\", \"Std. Error\", \"t value\", \"p-value\")],\n                            digits = 3,\n                            caption = \"Regression Coefficients for Hired Labor Model\")  %&gt;% \n kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"center\") %&gt;%\n  column_spec(1:5, background = \"white\")\n\n\ntotalR2 &lt;- total_model_sum$r.squared\nhiredR2 &lt;- hired_model_sum$r.squared\n\n# Print the tables and R^2\ntotal_labor_table\n\n\n\nRegression Coefficients for Total Labor Model\n\n\nVariable\nEstimate\nStd. Error\nt value\np-value\n\n\n\n\n(Intercept)\n2.095\n0.088\n23.837\n0.000\n\n\navg_pesticide\n0.065\n0.162\n0.404\n0.688\n\n\navg_fertilizer\n0.096\n0.078\n1.236\n0.224\n\n\nyears_elapsed\n-0.035\n0.008\n-4.381\n0.000\n\n\n\n\n\n\n\nCode\nprint(paste0(\"The R^2 for our total labor model is \", round(totalR2, 2), \".\"))\n\n\n[1] \"The R^2 for our total labor model is 0.9.\"\n\n\nCode\nhired_labor_table\n\n\n\nRegression Coefficients for Hired Labor Model\n\n\nVariable\nEstimate\nStd. Error\nt value\np-value\n\n\n\n\n(Intercept)\n1.767\n0.084\n20.908\n0.000\n\n\navg_pesticide\n0.298\n0.156\n1.918\n0.062\n\n\navg_fertilizer\n0.079\n0.075\n1.059\n0.296\n\n\nyears_elapsed\n-0.038\n0.008\n-4.920\n0.000\n\n\n\n\n\n\n\nCode\nprint(paste0(\"The R^2 for our hired labor model is \", round(hiredR2, 2), \".\"))\n\n\n[1] \"The R^2 for our hired labor model is 0.85.\"\n\n\n\nTotal labor model\nThe \\(R^2\\) for this model is 0.9, which indicates that it may be a good fit for our data, explaining 90% of the variance. The p-value for both pesticide and fertilizer in this model were much higher than our conventional threshold of \\(\\alpha\\) &lt; 0.05. This means that they are not statistically significant, and there is weak evidence that they affect total labor. The value for year was significant at 0.00, but the number is so low that it may suggest multicollinearity rather than strong significance.\n\n\nHired labor model\nThe \\(R^2\\) for this model is 0.85, which indicates that it, too may be a good fit for our data, explaining 85% of the variance. The p-value for fertilizer in this model was much higher than our conventional threshold of \\(\\alpha\\) &lt; 0.05. Even though pesticide had a value of 0.6, it is still not statistically significant. The value for year was once again 0.00, and our suspicions fo collinearity are reinforced, despite hte strong \\(R^2\\).\n\n\nAre the residuals from our model normally distributed?\nWe had assumed that our total labor data was normally distributed but our results are suggesting otherwise. Let’s look at a histogram and a Q-Q plot of the residuals to better visualize the distribution.\n\n\nCode\n# Extract residuals\nresiduals &lt;- total_labor_model$residuals\n\n# Plot a histogram of residuals to check normality\nggplot(data.frame(residuals), aes(x = residuals)) +\n  geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Histogram of Residuals\", x = \"Residuals\", y = \"Frequency\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Make a Q-Q plot to check normality\nggplot(data.frame(residuals), aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line(color = \"slategray\", alpha = 0.5) +\n  labs(title = \"Q-Q Plot of Residuals\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n\n\n\n\n\n\n\nThese plots indicate that the data produces from our model isn’t normally distributed after all! The histogram appears to be bimodal, with a tail to the right. The Q-Q plot shows that residuals are relatively close to the line of normality near median values, but deviate at both extremes. Both of these results suggest that the data is not normally distributed. We may have omitted variable bias, or perhaps there is a way to fit our model better.\nThe variance does not appear to be constant across all levels of the predictor variables— the points are much farther from the norm at lower x-values than they are at median or high values. This might mean that our data is heteroscedastic. Heteroscedasticity refers to conditions of a regression analysis when the size of the errors(variance) changes as the predictor variables increase or decrease.\n\n\nTesting for Heteroscedasticity\nWe suspect that our model violates a key assumption of Ordinary Least Squares (OLS) and we should confirm this so we can fit a better model. To test for heteroscedasticity, let’s plot our residuals against the fitted values. If the pattern curves, then our data is heteroscedastic and our model should be revised or abandoned.\n\n\nCode\n# Calculate fitted values and residuals\nfitted_values &lt;- fitted(total_labor_model)\nresidual_values &lt;- residuals(total_labor_model)\n\n# Create a data frame \nresiduals_df &lt;- data.frame(Fitted = fitted_values, Residuals = residual_values)\n\n# Create the plot\nggplot(residuals_df, aes(x = Fitted, y = Residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"salmon\", linetype = \"dashed\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs Fitted Values\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n\n\n\n\n\n\n\nIt looks like our suspicions were correct. We need to find a new way to analyze this data.\n\n\n\nbrewerint.com"
  },
  {
    "objectID": "blog_posts/stats-project/pesticides-fertilizers-labor.html#gamma-regression-analysis",
    "href": "blog_posts/stats-project/pesticides-fertilizers-labor.html#gamma-regression-analysis",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Gamma Regression Analysis",
    "text": "Gamma Regression Analysis\nWe know that both labor parameters are collinear as well as pesticide & fertilizer. Because it’s a redundant predictor, Let’s drop fertilizer and just focus on pesticide use for this next model. We’ve also learned that the data isn’t normal. To try and normalize the distribution of residuals, we need to transform our data. Though we know the increase in use of pesticides and fertilizers isn’t going to be exponential, the initial plot for their growth trend did somewhat reflect an exponential curve. Let’s try using a Generalized Linear Model (GLM), with a log link function to get a better model fit with a Gamma Regression.\n\n\\(\\log(\\mathbb{E}[\\text{avg\\_total\\_labor}]) = \\beta_0 + \\beta_1 \\cdot \\text{avg\\_pesticide} + \\beta_2 \\cdot \\text{years\\_elapsed} + \\beta_3 \\cdot (\\text{avg\\_pesticide} \\times \\text{years\\_elapsed})\\)\n\nThe regression coefficients for this model represent the following:\n\n\\(\\beta_0\\) = Our intercept. The ‘avg_total_labor’ value when all predictor variables are equal to zero.\n\\(\\beta_1\\) = The change in ‘avg_total_labor’ for each one unit increase in ‘avg_pesticide’.\n\\(\\beta_2\\) = The change in ‘avg_total_labor’ for each one unit increase in ‘years_elapsed’.\n\\(\\beta_3\\) = The quantification of how the effect of ‘avg_pesticide’ use on ‘avg_total_labor’ changes over time (as measured by ‘years_elapsed’).\n\n\n\nCode\n# Make a Gamma regression model for labor in relation to pesticide and years elapsed\nlabor_glm &lt;- glm(avg_total_labor ~ avg_pesticide + years_elapsed, \n                   family = Gamma(link = \"log\"), # Gamma distribution is used for modeling continuous, positive-valued data\n                   data = combined_tidy)\n\n\nNow that we’ve created our new model, let’s see the results. We’ll repeat the steps we took before to find the p-values.\n\n\nCode\n# Extract the summaries for the model\nlabor_glm_summary &lt;- summary(labor_glm)$coefficients\n\n# Convert to data frames for easier manipulation\nlabor_glm_df &lt;- as.data.frame(labor_glm_summary)\n\n# Populate the data frame with the variable names and their corresponding values\ncolnames(labor_glm_df) &lt;- c(\"Estimate\", \"Std. Error\", \"t value\", \"p-value\")\nlabor_glm_df$Variable &lt;- rownames(labor_glm_df)\nlabor_glm_df$Model &lt;- \"Gamma Labor\"\n\n# Remove row names (index) by resetting the row names to NULL\nrownames(labor_glm_df) &lt;- NULL\n\nlabor_glm_table &lt;- kable(labor_glm_df[, c(\"Variable\", \"Estimate\", \"Std. Error\", \"t value\", \"p-value\")],\n                            digits = 3,\n                            caption = \"Regression Coefficients for Labor Gamma Regression Model\") %&gt;% \n kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"center\") %&gt;%\n  column_spec(1:5, background = \"white\")\n\nlabor_glm_table\n\n\n\nRegression Coefficients for Labor Gamma Regression Model\n\n\nVariable\nEstimate\nStd. Error\nt value\np-value\n\n\n\n\n(Intercept)\n0.807\n0.020\n41.041\n0.000\n\n\navg_pesticide\n0.069\n0.071\n0.964\n0.341\n\n\nyears_elapsed\n-0.022\n0.003\n-6.353\n0.000\n\n\n\n\n\n\n\nProgress! It looks like our p-value for ‘avg_pesticide’ is roughly half of what our last model produced. This is a meaningless improvement however, as the new value is still statistically insignificant at \\(\\alpha\\) &lt; 0.05 with a value of 0.34. Let’s check to see if this new method produced data that is normal and homoscedastic, using the same methods from before.\n\nGamma Model Evaluation\n\n\nCode\n# Generate predictions and residuals for avg_total_labor using the gamma model\ncombined_glm &lt;- combined_tidy %&gt;%\n  mutate(predicted_labor = predict(labor_glm, type = \"response\")) %&gt;% \n  mutate(residuals_glm = residuals.glm(labor_glm))\n\n\n# Plot a histogram of gamma residuals to check normality\nglm_hist_plot &lt;- ggplot(combined_glm, aes(x = residuals_glm)) +\n  geom_histogram(bins = 30, fill = \"lavender\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Histogram of Gamma Regression Residuals\", x = \"Residuals\", y = \"Frequency\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n# Make a Q-Q plot to check normality\nglm_qq_plot &lt;- ggplot(combined_glm, aes(sample = predicted_labor)) +\n  stat_qq() +\n  stat_qq_line(color = \"firebrick\", alpha = 0.5) +\n  labs(title = \"Q-Q Plot of Gamma Regression Residuals\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n# Create the plot\nglm_fitted_resdid_plot &lt;- ggplot(combined_glm, aes(x = predicted_labor, y = residuals_glm)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"tomato\", linetype = \"dashed\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Gamma Regression Residuals vs Fitted Values\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n# Display the graphs\nglm_hist_plot\n\n\n\n\n\n\n\n\n\nCode\nglm_qq_plot\n\n\n\n\n\n\n\n\n\nCode\nglm_fitted_resdid_plot\n\n\n\n\n\n\n\n\n\nAfter further investigation, it’s clear that this model didn’t produce normally distributed data either. Our histogram of GLM residuals is closer to a normal distribution, but it’s asymmetric with a right tail. Our Q-Q plot of Gamma regression residuals is also a better fit, with at least half of the points landing on the line of normality. Still, the extreme high and low values deviate from the line. When we plotted the fitted Gamma data versus the residual Gamma data, the result is a funnel similar to the first graph. Though improved, this model did not produce normal or homoscedastic data and is therefore not worthwhile as a means to predict total labor cost from pesticide use and year alone.\n\n\nVisualizing Both Linear and Non-Linear Models\nLet’s compare each model by plotting our data and regression lines. Can we see a demonstrable difference in outcome?\n\n\nCode\n# Create the plot for Total Labor Model\ntotal_labor_plot &lt;- ggplot(combined_glm, aes(x = years_elapsed, y = avg_total_labor)) +\n  geom_point(color = \"slategray\", size = 2) +  # Scatter plot for avg_total_labor\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"firebrick\", linetype = \"solid\") +  # Regression line for total_labor_model\n  labs(subtitle = \"Linear Regression Model\", title = \"Total Labor Demand After 1960\", x = \"Years Past 1960\", y = \"Average Total Labor\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5), # Center the title\n        plot.subtitle = element_text(hjust = 0.5))  # Center the subtitle\n\n\n# Create the plot for Total Labor Gamma\ntotal_labor_glm_plot &lt;- ggplot(combined_glm, aes(x = years_elapsed, y = predicted_labor)) +\n  geom_point(color = \"forestgreen\", size = 2) +  # Scatter plot for avg_total_labor\n  geom_line(aes(y = predicted_labor),, color = \"slateblue\", linetype = \"solid\") +  # Regression line for total_labor_model\n  labs(subtitle = \"Gamma Regression Model\", title = \"Predicted Total Labor Demand \\nAfter 1960\", x = \"Years Past 1960\", y = \"Predicted Average Total Labor\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5), # Center the subtitle\n        plot.subtitle = element_text(hjust = 0.5))  # Center the title\n\n# Display both plots side by side\nprint(total_labor_plot)  \n\n\n\n\n\n\n\n\n\nCode\nprint(total_labor_glm_plot)"
  },
  {
    "objectID": "blog_posts/stats-project/pesticides-fertilizers-labor.html#results",
    "href": "blog_posts/stats-project/pesticides-fertilizers-labor.html#results",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Results",
    "text": "Results\nThrough our correlation analysis, we found that average national agricultural pesticide and fertilizer use has exploded since 1960. These two variables are closely correlated, with \\(r\\) values of 0.82. This suggests that most growers who use fertilizers will also rely on pesticides in their approach to farming.\nOur linear regression analysis also found that labor demand has dropped significantly in the same time, less than half what it once was. The resulting negative correlation between labor demand and synthetic inputs is not so clear cut, however. When looking at any other predictor variable, years elapsed from 1960 had extremely strong correlation, positive or negative.\nFurther investigation of the efficacy of our model suggest that it is a poor fit to draw any conclusions from. When plotted, the data did not have a normal distribution, the variance was not consistent, diverging at extreme values, and the coefficient p-values were not of significance.\nA revised gamma regression had slightly more convincing results, but still nothing concrete. We were able to get our coefficient p-value to half of what it was initially, and the data was more normally distributed, but it remained heteroscedastic and the p-value was still insignificant despite being lower.\nLooking back at our Null and Alternative hypotheses, it’s impossible to be conclusive. Based on our findings, we fail to reject our null hypothesis that pesticide and fertilizer use do not have a significant effect on labor demand. Both of our predictive models yielded coefficient p-values notably higher than the convention for significance of \\(alpha\\) &lt; 0.05.\nOur results suggest that there are other factors not included in our models that have a significant impact on agricultural labor demand. This is known as Omitted variable bias. Omitted variable bias occurs when a relevant predictive variable is excluded from a model, resulting in biased estimates for the included variables’ coefficients. This happens because the excluded variable is correlated with both the dependent variable and one or more of the independent variables, leading to inaccurate projections.\nDespite our findings that neither model is a perfect fit, we can still learn something from them. Visualized, we can see that our GLM on the right does provide a better fit for our scenario. This means that labor demand is not a fixed linear variable, it decays over time. Though not dramatic, our predictive line does curve slightly. This suggests that labor demand is influenced by factors that affect it’s trend of decay as well as the demand itself.\n\nAdditional Considerations & Possible Next Steps\nTo improve on this analysis in the future, omitted variables need to be identified and included in the gamma regression model. There are likely more than one significant predictors that are closely correlated to our ‘years_elapsed’ variable.\n\n\n\nextension.umn.edu\n\n\nThese might include the introduction of cost-saving technologies, higher yielding and pest or disease resistant crops, higher domestic wages, cost of living changes, inflation, an increase in availability of cheaper migrant labor, broader economic trends, or other unidentified predictors. It is possible that one or more of these factors has relevance to the model, and including them w would further mitigate omitted variable bias and improve the interpretability and accuracy of the model.\nFurthermore, more localized, targeted data might be helpful in understanding trends. Comparing data from different regions or states and including relevant economic factors could aid in evaluating the connection between synthetic inputs and labor if there is one. A comparison between conventional and organic farms would also be valuable, though the relatively small number and size of the latter would be an obstacle to overcome.\n\n\nReferences\n1: U.S. Department of Agriculture, Economic Research Service. (2024). Agricultural productivity in the United States. https://www.ers.usda.gov/data-products/agricultural-productivity-in-the-united-states/. Accessed: November 11, 2024.\n2: U.S. Department of Agriculture, Economic Research Service. (2024). Fertilizer & Pesticide Use in the United States https://www.ers.usda.gov/webdocs/DataFiles/47679/table17.xls?v=0. Accessed: November 11, 2024.\n\n\nData Availability\nThe report Fertilizer & Pesticide Use in the United States is no longer available online at the source above. The data can be found in the repository for this post.\n\n\nAcknowledgements\nThis post is based on a project from Max Czapanskiy for EDS-222: Statistics for Environmental Data Scientists."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stephen Carroll",
    "section": "",
    "text": "Howdy, I’m Stephen!\nI am a recent MEDS graduate in search of problems to solve.\nI’m interested in all things data science, agroecology, and conservation. When I’m not crunching numbers, you can find me in the garden composting, the kitchen simmering, or the cellar fermenting."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Stephen Carroll",
    "section": "Education",
    "text": "Education\nMaster of Environmental Data Science (MEDS), Bren School of Environmental Science & Management, University of California, Santa Barbara (2025)\nCertificate, Microscopy & Soil Science - Soil Food Web School (2022)\nB.A. Anthropology - University of California, Santa Cruz (2014)"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Stephen Carroll",
    "section": "Skills",
    "text": "Skills\nMachine Learning & Data Analysis: Python(NumPy, Pandas, Matplotlib, Seaborn, Time Series, Machine Learning, Decision Trees, Parameter Tuning), R(tidyverse), SQL(SQLite, PostgreSQL), Git/GitHub, ArcGIS\nData Visualization: Python(Matplotlib, Seaborn), R(ggplot2, tmap, shiny\nComputer: Microsoft & Google suites, QuickBooks, Canva, Mailchimp, Power BI, Tableau"
  },
  {
    "objectID": "safe.html",
    "href": "safe.html",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "",
    "text": "cnbc.com\nThe repository for this analysis can be accessed here."
  },
  {
    "objectID": "safe.html#introduction",
    "href": "safe.html#introduction",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Introduction",
    "text": "Introduction\nThe purpose of this post is to analyze the relationship between annual synthetic inputs in commercial agriculture and corresponding labor demands. It aims to examine the pesticide and fertilizer use by farms in the United States between 1960 and 2004, as well as labor needs over the same parameters, and determine how these factors affect one another."
  },
  {
    "objectID": "safe.html#background",
    "href": "safe.html#background",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Background",
    "text": "Background\nMost people don’t think much about the work that goes in to producing the food that they eat. I know I didn’t. Like many of us, I grew up disconnected from the source of my food, unaware of the hard work that brings it to our tables. It wasn’t until I became involved in agriculture that I truly understood the effort and resources required to grow the food we often take for granted. Throughout history, labor has consistently been one of the largest expenses in agriculture, both in terms of time and financial investment. From planting and cultivating to harvesting and processing crops, labor is essential to ensuring that food reaches our markets.\nI never really considered just how important it was to the economics of a farming operation until I began working for a small organic farm in Santa Cruz County, California. One of my responsibilities was to pay our vendors, and I learned that the cost of inputs—seed, fertilizer, amendments, etc.—was something that farmers are always trying to reduce. It was much harder to reduce labor. In the peak of Summer, our payroll could be nearly twice as much as it was in the Winter! That’s no small fluctuation, and it put a lot of stress on the business to operate smoothly.\nI remember my shock when I learned that our crops were not only weeded with a cultivating tractor, but by hand as well. Several times a week, our crew of more than 20 would spend hours pulling weeds in a block of lettuce or scallions. The weeds that grew too close to the crops to be uprooted by the tillage blades of the tractor were a big concern. If they didn’t cull them in time, they would outgrow the crops and the harvest would take much longer, making it cost-ineffective. During the busy Summer months when weeds grow faster and we had tomatoes and strawberries to harvest, we would occasionally lose a block of something and have to cut our losses and till the land, preparing it for a new planting.\n\n\n\nfarmworkerjustice.org\n\n\nOther times, we would be plagued with an onslaught of insects. Flea beetles that would eat holes through our lettuce leaves and blemish our radish greens. Wireworms that tunneled into our carrots and made them bitter. Fruit flies, that would feast on rotting berries and multiply by the millions if furrows weren’t quickly and meticulously cleared of any discarded fruit. One year we couldn’t keep up with the cleaning and the flies became an infestation that curtailed the entire season and spoiled the appetite of some customers. Of course, if we weren’t organic, we would have sprayed for flies and been able to continue to harvest fruit into October. It’s a bitter truth, and it caused me to reflect on conventional farms that use synthetic chemicals to control weeds and pests. No wonder organic produce is so expensive, I’d think. This realization prompted me to consider how the demand for labor in agriculture has evolved, particularly in response to the rising use of synthetic pesticides and fertilizers. I began to wonder: was labor cost minimization due to these inputs the reason large conventional farms were successful, when smaller organic operations seem to come and go?\nThis curiosity led me to explore the relationship between pesticide and fertilizer use and labor trends in agriculture. In this post, I will examine how the increasing reliance on synthetic chemicals has reshaped labor needs in the U.S.A. since 1960. Using statistical analysis, I will explore how labor costs have changed as farming practices evolved, offering insights into the broader economic and social impacts of these shifts. Through this analysis, I hope to better understand the relationship between these factors, the trajectory of labor in agriculture and the ongoing challenges to success it presents."
  },
  {
    "objectID": "safe.html#analysis-plan",
    "href": "safe.html#analysis-plan",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Analysis Plan",
    "text": "Analysis Plan\n\nMultiple Linear Regression\nTo better understand the relationship between pesticides, fertilizers, labor and time, We’ll look at historical data from 1960-2004. This period saw a dramatic rise in these synthetic inputs, so it should allow a good perspective on the relationship. Annual usage data from the US Department of Agriculture will allow us to determine the rate of this change, and annual national labor use data should give us an idea of the effect it had on farms.\nWe’ll analyze the correlation of these factors over time, which will give us an indication of their relationship. Next, I’ll implement linear regression analysis to determine the effect that pesticide and fertilizer use, and time had on labor needs. The regression model will be the following:\n\\[\n\\text{labor} = \\beta_0 + \\beta_1 \\cdot \\text{pesticide} + \\beta_2 \\cdot \\text{fertilizer} + \\beta_3 \\cdot \\text{year} + \\epsilon\n\\]\nThe response variable is ‘total_labor’, and represents all forms of labor demand in commercial agriculture operations in the country. The \\(\\beta\\)s represent the regression coefficients, each representing a predictor variable, all of which are continuous. This assumes that there is a linear relationship between ‘total_labor’ and each independent predictor variable.\nThis model will enable me to assess the extent to which changes in these factors predict the demand in labor. By examining these patterns over time we can attempt to draw conclusions about how the increasing reliance on synthetic chemicals has influenced labor dynamics in agriculture, potentially providing valuable insight into the economic impacts of different farming practices.\n\n\nLogistic Regression\nThough I don’t believe my variables to be categorical, if linear regression does not yield convincing results, I will turn to logistic analysis. Having a secondary method of analysis may prove valuable if my predictor variables are insufficient to draw conclusions. Trying both models allows me to compare their performance and ensure I’m using the most appropriate approach for my analysis. Herre is an example of a model that I might use:\n\\[\\log(\\mathbb{E}[\\text{labor}]) = \\beta_0 + \\beta_1 \\cdot \\text{pesticide} + \\beta_2 \\cdot \\text{year} + \\beta_3 \\cdot (\\text{pesticide} \\times \\text{year})\\]\n\n\nHypothesis Statement\nNull Hypothesis (H₀): Pesticide and fertilizer use do not have a significant effect on labor demand.\nAlternative Hypothesis (H₁): Pesticide and fertilizer use have a significant effect on labor needs."
  },
  {
    "objectID": "safe.html#data",
    "href": "safe.html#data",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Data",
    "text": "Data\nFor this analysis, all data will be sourced from the United States Department of Agriculture (USDA) Economic Research Service.\nThe labor data comes from a report titled Agricultural productivity in the United States., and contains annual state and U.S. average statistics of agricultural inputs and outputs, including several classifications of labor indices. Indices of labor input were constructed for each State and the aggregate farm sector using the demographically cross-classified hours and compensation data. Labor hours having higher marginal productivity (wages) are given higher weights in forming the index of labor input than are hours having lower marginal productivities. Doing so explicitly adjusts indices of labor input for “quality” change in labor hour.[1] For this analysis, we will focus on the total and hired labor indices, values which are best suited to represent the labor that would be affected by changes in other inputs.\nThe pesticide and fertilizer data originates from **Fertilizer & Pesticide Use in the United States* summarizing surveyed states annual consumption of material inputs from 1960 to 2004. Indices are a relative to the rate of consumption in Alabama in 1996 based on a value of 1.\nThe data is formatted as several tables per document with headers and notes, and required reformatting, filtering, and removal of unnecessary summary tables in order to utilize further."
  },
  {
    "objectID": "safe.html#analysis",
    "href": "safe.html#analysis",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Analysis",
    "text": "Analysis\n\n\n\ncropaia.com\n\n\n\nSetup\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(here)\nlibrary(readxl)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(gridExtra)\n\n\n\n\nPesticide & Fertilizer Data Extraction\nThese two dataframes are from the same source and have identical structure. First, we have to read in the raw data for our synthetic inputs; Pesticides & Fertilizers. The data is not in a usable format, so we will have to remove anything extraneous. This allows us pivot the dataframe longer so that it’s easier to conduct analysis. We need to find the national average pesticide usage and put that value into a new column. We’ll do the same thing for our fertilizer data below.\n\n\nCode\n# Read in pesticide data\npesticide &lt;- read_excel(here(\"blog_posts/stats-project/data/pesticide_consumption_state.xls\"), skip = 5, range = \"A6:AW53\")\n\n# Remove rows that are entirely blank\npesticide_clean &lt;- pesticide[rowSums(is.na(pesticide)) &lt; ncol(pesticide), ]\n\npesticide_tidy &lt;- pesticide_clean %&gt;%\n  pivot_longer(cols = -Year, # Pivot all columns except Year\n               names_to = \"state\", # Create a new column for each state \n               values_to = \"pesticide\") %&gt;% # The index values \n  group_by(Year) %&gt;%\n  mutate(avg_pesticide = mean(pesticide)) %&gt;%  # Create a new column for mean use (National average)\n  ungroup() %&gt;%\n  distinct(Year, .keep_all = TRUE) %&gt;% # Remove duplicate rows by year\n  select(Year, avg_pesticide)  # Keep only 'Year' and 'us_avg_pesticide'\n\n\n\n\nCode\n# Read in fertilizer data\nfertilizer &lt;- read_excel(here(\"blog_posts/stats-project/data/fertilizer_consumption_state.xls\"), skip = 5, range = \"A6:AW53\")\n\n# Remove rows that are entirely blank\nfertilizer_clean &lt;- fertilizer[rowSums(is.na(fertilizer)) &lt; ncol(fertilizer), ]\n\nfertilizer_tidy &lt;- fertilizer_clean %&gt;%\n  pivot_longer(cols = -Year, # Pivot all columns except Year\n               names_to = \"state\",# Create a new column for each state\n               values_to = \"fertilizer\") %&gt;% # The index values \n  group_by(Year) %&gt;%\n  mutate(avg_fertilizer = mean(fertilizer)) %&gt;%  # Create a new column for mean use (National average)\n  ungroup() %&gt;%\n  distinct(Year, .keep_all = TRUE) %&gt;% # Remove duplicate rows by year\n  select(Year, avg_fertilizer)  # Keep only 'Year' and 'avg_fertilizer'\n\n\n\n\nLabor Data Extraction:\nNow we can process our labor data. This information is part of a diverse dataframe that has a lot of information that’s not helpful for us to look at. Let’s simplify it by filtering for observations with ‘labor’ in the title. Next we can pivot the table longer to match the format of our other data, with each attribute having its own column. Since this data is annual, we don’t need to make a new column for annual average. Now that our data is reformatted, we can filter it. We can remove the column for self-employed and family workers because these people would likely work for the farms regardless of changes in pesticide and fertilizer use.\n\n\nCode\n# Read in labor data\nlabor &lt;- read_csv(here(\"blog_posts/stats-project/data/labor_etc.csv\"))\n\n# Filter rows in the 'labor' data frame where any column contains the word \"labor\"\nfiltered_labor &lt;- labor[apply(labor, 1, function(row) any(grepl(\"labor\", row, ignore.case = TRUE))), ]\n\n# Pivot the data so that each attribute becomes its own column\nlabor_clean &lt;- filtered_labor %&gt;%\n  pivot_wider(\n    names_from = Attribute, # The column to spread into multiple columns\n    values_from = Value # The column to use as the values for each new column\n  )\n\n# Subset the labor data to keep only total labor index and hired labor index\nlabor_tidy &lt;- labor_clean[, c(\"Labor inputs: Total\", \"Labor inputs: Hired labor\", \"Year\"), drop = FALSE] %&gt;%\n  select(\"Year\", \"Labor inputs: Total\", \"Labor inputs: Hired labor\") %&gt;%\n   filter(Year &gt;= 1960) %&gt;%  # Filter for our area of interest\n  rename(\n    avg_total_labor = \"Labor inputs: Total\", # Rename the column for total labor\n    avg_hired_labor = \"Labor inputs: Hired labor\" # Rename the column for hired labor\n  )\n\n\n\n\nMerge Data Together\nWe now have three datasets that have similar format and a common column for ‘Year’. Let’s join them together to create a single daatset that we can use for the entire analysis. We’ll also add a column to indicate the amount of years elapsed from the beginning of our data, 1960.\n\n\nCode\n# Merge the pesticide_tidy, fertilizer_tidy, and labor_tidy dataframes by the 'Year' column\ncombined_tidy &lt;- left_join(pesticide_tidy, fertilizer_tidy, by = \"Year\") %&gt;%\n  left_join(labor_tidy, by = \"Year\")\n\n# Add a column for the time difference between each year and the previous year\ncombined_tidy &lt;- combined_tidy %&gt;%\n  mutate(years_elapsed = Year - min(Year))\n\n\nLet’s take a look at our tidy new dataset:\n\n\nCode\n# View the updated dataframe\nhead(combined_tidy)\n\n\n# A tibble: 6 × 6\n   Year avg_pesticide avg_fertilizer avg_total_labor avg_hired_labor\n  &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n1  1960         0.218          0.948            2.50            2.12\n2  1961         0.247          0.986            2.44            2.12\n3  1962         0.270          0.939            2.45            2.11\n4  1963         0.279          1.06             2.35            2.11\n5  1964         0.298          1.18             2.21            1.90\n6  1965         0.296          1.25             2.15            1.79\n# ℹ 1 more variable: years_elapsed &lt;dbl&gt;"
  },
  {
    "objectID": "safe.html#data-visualization-exploration",
    "href": "safe.html#data-visualization-exploration",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Data Visualization & Exploration",
    "text": "Data Visualization & Exploration\nTo get a better idea of the usage trends for our synthetic inputs, we can make line graphs with ‘Year’ on the x-axis and the index value on the y-axis. When we display them we can see clearly that both pesticide and fertilizer use has risen dramatically in the U.S. since 1960.\n\n\nCode\n# Plot the trend of pesticide use over time\npest_trend_plot &lt;- ggplot(combined_tidy, aes(x = Year, y = avg_pesticide)) +\n  geom_line(color = \"orange\", size = 1.5, alpha = 0.7) +  # Add color, size, and transparency\n  labs(\n    title = \"Average Pesticide Use Over Time\",\n    subtitle = \"Trend of U.S. Average Pesticide Use by Year (1960-2004)\",\n    x = \"Year\",\n    y = \"Average Pesticide Use (Index)\",\n    caption = \"Source: U.S. Department of Agriculture, Economic Research Service. \"  # Caption for data source\n  ) +\n  theme_minimal() +  # Clean background\n  theme(\n    text = element_text(size = 12),  # Set base font size\n    axis.title = element_text(face = \"bold\"),  # Bold axis titles\n    axis.text = element_text(color = \"black\"),  # Color for axis labels\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),  # Centered title with larger font\n    plot.subtitle = element_text(hjust = 0.5, size = 12),  # Centered subtitle\n    plot.caption = element_text(hjust = 0, size = 10, face = \"italic\")  # Caption style\n  )\n\n# Plot the trend of pesticide use over time\nfert_trend_plot &lt;- ggplot(combined_tidy, aes(x = Year, y = avg_fertilizer)) +\n  geom_line(color = \"darkgreen\", size = 1.5, alpha = 0.7) +  # Add color, size, and transparency\n  labs(\n    title = \"Average Fertilizer Use Over Time\",\n    subtitle = \" Trend of U.S. Average Fertilizer Use (1960-2004)\",\n    x = \"Year\",\n    y = \"Average Fertilizer Use (Index)\",\n    caption = \"Source: U.S. Department of Agriculture, Economic Research Service. \"  # Caption for data source\n  ) +\n  theme_minimal() +  # Clean background\n  theme(\n    text = element_text(size = 12),  # Set base font size\n    axis.title = element_text(face = \"bold\"),  # Bold axis titles\n    axis.text = element_text(color = \"black\"),  # Color for axis labels\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),  # Centered title with larger font\n    plot.subtitle = element_text(hjust = 0.5, size = 12),  # Centered subtitle\n    plot.caption = element_text(hjust = 0, size = 10, face = \"italic\")  # Caption style\n  )\n\n# Display graphs next to each other\nfert_trend_plot + pest_trend_plot + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\nNow, let’s determine the trend for our labor variables. We’ll plot them together on the same graph because their scale is the same. When we do we can see that both hired and total labor has decreased precipitously since 1960.\n\n\nCode\n# Plot the trend of labor needs over time\nggplot(combined_tidy, aes(x = Year)) +\n  geom_line(aes(y = avg_total_labor, color = \"Total Labor\"), size = 1.5) +  \n  geom_line(aes(y = avg_hired_labor, color = \"Hired Labor\"), size = 1.5) + \n  scale_color_manual(values = c(\"Total Labor\" = \"blue\", \"Hired Labor\" = \"red\")) +  \n  labs(\n    title = \"Labor Inputs: Total vs. Hired Labor Over Time\",\n    subtitle = \"Trend of U.S. Average Total Labor and Hired Labor Input Indices (1960-2004)\",\n    x = \"Year\",\n    y = \"Labor Inputs (Index)\",\n    color = \"Labor Type\"  # Legend title\n  ) +\n  theme_minimal() +  # Clean theme\n  theme(\n    text = element_text(size = 12),  # Set base font size\n    axis.title = element_text(face = \"bold\"),  # Bold axis titles\n    axis.text = element_text(color = \"black\"),  # Axis labels color\n    legend.title = element_text(face = \"bold\"),  # Bold legend title\n    legend.position = \"top\",  # Position the legend at the top\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),  # Centered plot title with larger font size\n    plot.subtitle = element_text(hjust = 0.5, size = 12)  # Centered subtitle\n  )\n\n\n\n\n\n\n\n\n\nTo further understand the relationship between each variable, let’s make a correlation matrix. The value for each cell represents \\(r\\), the correlation coefficient. An \\(r\\) value ranges from -1 to 1, with 0 indicating no correlation at all. A value of -1 means that the variables are a perfect negative linear relationship, white a value of 1 represents a perfect positive relationship.\n\n\nCode\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(combined_tidy[, c(\"avg_pesticide\", \"avg_fertilizer\", \"avg_total_labor\", \"avg_hired_labor\", \"years_elapsed\")])\n\n# Convert the correlation matrix into a data frame for better table formatting\ncor_df &lt;- as.data.frame(cor_matrix)\n\n# Use kable() to display the correlation matrix as a table\nkable(cor_df, digits = 2, caption = \"Correlation Matrix of Agricultural Inputs & Labor Categories (r)\") %&gt;% \n kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"center\") %&gt;%\n  column_spec(1:6, background = \"white\")\n\n\n\nCorrelation Matrix of Agricultural Inputs & Labor Categories (r)\n\n\n\navg_pesticide\navg_fertilizer\navg_total_labor\navg_hired_labor\nyears_elapsed\n\n\n\n\navg_pesticide\n1.00\n0.82\n-0.93\n-0.87\n0.98\n\n\navg_fertilizer\n0.82\n1.00\n-0.76\n-0.72\n0.83\n\n\navg_total_labor\n-0.93\n-0.76\n1.00\n0.96\n-0.95\n\n\navg_hired_labor\n-0.87\n-0.72\n0.96\n1.00\n-0.91\n\n\nyears_elapsed\n0.98\n0.83\n-0.95\n-0.91\n1.00\n\n\n\n\n\n\n\nLet’s make scatter plots as well to better visualize these relationships and confirm the results from the correlation matrix.\n\n\nCode\n#  Pesticide vs Fertilizer Use\ncor_plot1 &lt;- ggplot(combined_tidy, aes(x = avg_pesticide, y = avg_fertilizer)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"cornflowerblue\") + # Line of best fit\n  labs(title = \"Pesticide vs Fertilizer Use\", x = \"Average Pesticide\", y = \"Average Fertilizer\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Total Labor vs Hired Labor\ncor_plot2 &lt;- ggplot(combined_tidy, aes(x = avg_total_labor, y = avg_hired_labor)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"cornflowerblue\") + # Line of best fit\n  labs(title = \"Total Labor vs Hired Labor\", x = \"Average Total Labor\", y = \"Average Hired Labor\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Pesticide Use vs Total Labor\ncor_plot3 &lt;- ggplot(combined_tidy, aes(x = avg_pesticide, y = avg_total_labor)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") + # Line of best fit\n  labs(title = \"Pesticide Use vs Total Labor\", x = \"Average Pesticide\", y = \"Average Total Labor\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Fertilizer Use vs Hired Labor\ncor_plot4 &lt;- ggplot(combined_tidy, aes(x = avg_fertilizer, y = avg_hired_labor)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") + # Line of best fit\n  labs(title = \"Fertilizer Use vs Hired Labor\", x = \"Average Fertilizer\", y = \"Average Hired Labor\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Arrange all plots in a 2x2 grid\ncor_plot1 + cor_plot2 + cor_plot3 + cor_plot4 + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\nWe can see from these visualizations that pesticide and fertilizer both have a strong positive correlation with each other, and strong negative correlation with both labor variables. The labor variables have exactly the same extremely high correlation with each other, it appears that they are collinear and may not provide distinct results in other analysis. Because they are essentially redundant, they probably won’t be effective in our model."
  },
  {
    "objectID": "safe.html#linear-regression-analysis",
    "href": "safe.html#linear-regression-analysis",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Linear Regression Analysis",
    "text": "Linear Regression Analysis\nLet’s revisit the multiple linear regression model previewed earlier and update it with our specific variable names:\n\n\\(\\text{avg\\_total\\_labor} = \\beta_0 + \\beta_1 \\cdot \\text{avg\\_pesticide} + \\beta_2 \\cdot \\text{avg\\_fertilizer} + \\beta_3 \\cdot \\text{years\\_elapsed} + \\epsilon\\)\n\nNow we can define the model in more depth given what we’ve learned about our predictor variables. The regression coefficients represent the following:\n\n\\(\\beta_0\\) = Our intercept. The ‘avg_total_labor’ value when all predictor variables are equal to zero.\n\\(\\beta_1\\) = The change in ‘avg_total_labor’ for each one unit increase in ‘avg_pesticide’.\n\\(\\beta_2\\) = The change in ‘avg_total_labor’ for each one unit increase in ‘avg_fertilizer’.\n\\(\\beta_3\\) = The change in ‘avg_total_labor’ for each one unit increase in ‘Year’.\n\n\n\nCode\n# Make a multi linear regression model predicting total labor\ntotal_labor_model &lt;- lm(avg_total_labor ~ avg_pesticide + avg_fertilizer + years_elapsed, data = combined_tidy)\n\n# Get the summary of the model to interpret the coefficients\ntotal_model_sum &lt;- summary(total_labor_model)\n\n\nWe can make a nearly identical model to predict ‘hired_labor’. Let’s do that now so we can test the results together. For this model, the regression coefficients represent the same predictors, but they are predicting ‘avg_hired_labor’ now.\n\n\nCode\n# A separate model predicting avg_total_labor using only avg_hired_labor\nhired_labor_model &lt;- lm(avg_hired_labor ~ avg_pesticide + avg_fertilizer + years_elapsed, data = combined_tidy)\n\n# Get the summary of the model to interpret the coefficients\nhired_model_sum &lt;- summary(hired_labor_model)\n\n\n\nLinear Model Evaluation\nNow that we have summaries for both of our models, we can interpret them. To visualize the coefficient better, let’s put the results in a table.\n\n\nCode\n# Extract the summaries for both models\ntotal_labor_summary &lt;- summary(total_labor_model)$coefficients\nhired_labor_summary &lt;- summary(hired_labor_model)$coefficients\n\n# Convert to data frames for easier manipulation\ntotal_labor_df &lt;- as.data.frame(total_labor_summary)\ncolnames(total_labor_df) &lt;- c(\"Estimate\", \"Std. Error\", \"t value\", \"p-value\")\ntotal_labor_df$Variable &lt;- rownames(total_labor_df)\ntotal_labor_df$Model &lt;- \"Total Labor\"\n\nhired_labor_df &lt;- as.data.frame(hired_labor_summary)\ncolnames(hired_labor_df) &lt;- c(\"Estimate\", \"Std. Error\", \"t value\", \"p-value\")\nhired_labor_df$Variable &lt;- rownames(hired_labor_df)\nhired_labor_df$Model &lt;- \"Hired Labor\"\n\n# Remove row names (index) by resetting the row names to NULL\nrownames(total_labor_df) &lt;- NULL\nrownames(hired_labor_df) &lt;- NULL\n\n# Print the tables separately\ntotal_labor_table &lt;- kable(total_labor_df[, c(\"Variable\", \"Estimate\", \"Std. Error\", \"t value\", \"p-value\")],\n                           digits = 3, \n                           caption = \"Regression Coefficients for Total Labor Model\")  %&gt;% \n kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"center\") %&gt;%\n  column_spec(1:5, background = \"white\")\n\nhired_labor_table &lt;- kable(hired_labor_df[, c(\"Variable\", \"Estimate\", \"Std. Error\", \"t value\", \"p-value\")],\n                            digits = 3,\n                            caption = \"Regression Coefficients for Hired Labor Model\")  %&gt;% \n kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"center\") %&gt;%\n  column_spec(1:5, background = \"white\")\n\n\ntotalR2 &lt;- total_model_sum$r.squared\nhiredR2 &lt;- hired_model_sum$r.squared\n\n# Print the tables and R^2\ntotal_labor_table\n\n\n\nRegression Coefficients for Total Labor Model\n\n\nVariable\nEstimate\nStd. Error\nt value\np-value\n\n\n\n\n(Intercept)\n2.095\n0.088\n23.837\n0.000\n\n\navg_pesticide\n0.065\n0.162\n0.404\n0.688\n\n\navg_fertilizer\n0.096\n0.078\n1.236\n0.224\n\n\nyears_elapsed\n-0.035\n0.008\n-4.381\n0.000\n\n\n\n\n\n\n\nCode\nprint(paste0(\"The R^2 for our total labor model is \", round(totalR2, 2), \".\"))\n\n\n[1] \"The R^2 for our total labor model is 0.9.\"\n\n\nCode\nhired_labor_table\n\n\n\nRegression Coefficients for Hired Labor Model\n\n\nVariable\nEstimate\nStd. Error\nt value\np-value\n\n\n\n\n(Intercept)\n1.767\n0.084\n20.908\n0.000\n\n\navg_pesticide\n0.298\n0.156\n1.918\n0.062\n\n\navg_fertilizer\n0.079\n0.075\n1.059\n0.296\n\n\nyears_elapsed\n-0.038\n0.008\n-4.920\n0.000\n\n\n\n\n\n\n\nCode\nprint(paste0(\"The R^2 for our hired labor model is \", round(hiredR2, 2), \".\"))\n\n\n[1] \"The R^2 for our hired labor model is 0.85.\"\n\n\n\nTotal labor model\nThe \\(R^2\\) for this model is 0.9, which indicates that it may be a good fit for our data, explaining 90% of the variance. The p-value for both pesticide and fertilizer in this model were much higher than our conventional threshold of \\(\\alpha\\) &lt; 0.05. This means that they are not statistically significant, and there is weak evidence that they affect total labor. The value for year was significant at 0.00, but the number is so low that it may suggest multicollinearity rather than strong significance.\n\n\nHired labor model\nThe \\(R^2\\) for this model is 0.85, which indicates that it, too may be a good fit for our data, explaining 85% of the variance. The p-value for fertilizer in this model was much higher than our conventional threshold of \\(\\alpha\\) &lt; 0.05. Even though pesticide had a value of 0.6, it is still not statistically significant. The value for year was once again 0.00, and our suspicions fo collinearity are reinforced, despite hte strong \\(R^2\\).\n\n\nAre the residuals from our model normally distributed?\nWe had assumed that our total labor data was normally distributed but our results are suggesting otherwise. Let’s look at a histogram and a Q-Q plot of the residuals to better visualize the distribution.\n\n\nCode\n# Extract residuals\nresiduals &lt;- total_labor_model$residuals\n\n# Plot a histogram of residuals to check normality\nggplot(data.frame(residuals), aes(x = residuals)) +\n  geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Histogram of Residuals\", x = \"Residuals\", y = \"Frequency\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Make a Q-Q plot to check normality\nggplot(data.frame(residuals), aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line(color = \"slategray\", alpha = 0.5) +\n  labs(title = \"Q-Q Plot of Residuals\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n\n\n\n\n\n\n\nThese plots indicate that the data produces from our model isn’t normally distributed after all! The histogram appears to be bimodal, with a tail to the right. The Q-Q plot shows that residuals are relatively close to the line of normality near median values, but deviate at both extremes. Both of these results suggest that the data is not normally distributed. We may have omitted variable bias, or perhaps there is a way to fit our model better.\nThe variance does not appear to be constant across all levels of the predictor variables— the points are much farther from the norm at lower x-values than they are at median or high values. This might mean that our data is heteroscedastic. Heteroscedasticity refers to conditions of a regression analysis when the size of the errors(variance) changes as the predictor variables increase or decrease.\n\n\nTesting for Heteroscedasticity\nWe suspect that our model violates a key assumption of Ordinary Least Squares (OLS) and we should confirm this so we can fit a better model. To test for heteroscedasticity, let’s plot our residuals against the fitted values. If the pattern curves, then our data is heteroscedastic and our model should be revised or abandoned.\n\n\nCode\n# Calculate fitted values and residuals\nfitted_values &lt;- fitted(total_labor_model)\nresidual_values &lt;- residuals(total_labor_model)\n\n# Create a data frame \nresiduals_df &lt;- data.frame(Fitted = fitted_values, Residuals = residual_values)\n\n# Create the plot\nggplot(residuals_df, aes(x = Fitted, y = Residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"salmon\", linetype = \"dashed\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs Fitted Values\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n\n\n\n\n\n\n\nIt looks like our suspicions were correct. We need to find a new way to analyze this data.\n\n\n\nbrewerint.com"
  },
  {
    "objectID": "safe.html#logistic-regression-analysis",
    "href": "safe.html#logistic-regression-analysis",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Logistic Regression Analysis",
    "text": "Logistic Regression Analysis\nWe know that both labor parameters are collinear as well as pesticide & fertilizer. Because it’s a redundant predictor, Let’s drop fertilizer and just focus on pesticide use for this next model. We’ve also learned that the data isn’t normal. To try and normalize the distribution of residuals, we need to transform our data. Though we know the increase in use of pesticides and fertilizers isn’t going to be exponential, the initial plot for their growth trend did somewhat reflect an exponential curve. Let’s try using a Generalized Linear Model (GLM), with a log link function to get a better model fit.\n\n\\(\\log(\\mathbb{E}[\\text{avg\\_total\\_labor}]) = \\beta_0 + \\beta_1 \\cdot \\text{avg\\_pesticide} + \\beta_2 \\cdot \\text{years\\_elapsed} + \\beta_3 \\cdot (\\text{avg\\_pesticide} \\times \\text{years\\_elapsed})\\)\n\nThe regression coefficients for this model represent the following:\n\n\\(\\beta_0\\) = Our intercept. The ‘avg_total_labor’ value when all predictor variables are equal to zero.\n\\(\\beta_1\\) = The change in ‘avg_total_labor’ for each one unit increase in ‘avg_pesticide’.\n\\(\\beta_2\\) = The change in ‘avg_total_labor’ for each one unit increase in ‘years_elapsed’.\n\\(\\beta_3\\) = The quantification of how the effect of ‘avg_pesticide’ use on ‘avg_total_labor’ changes over time (as measured by ‘years_elapsed’).\n\n\n\nCode\n# Make a Gamma regression model for labor in relation to pesticide and years elapsed\nlabor_glm &lt;- glm(avg_total_labor ~ avg_pesticide + years_elapsed, \n                   family = Gamma(link = \"log\"), # Gamma distribution is used for modeling continuous, positive-valued data\n                   data = combined_tidy)\n\n\nNow that we’ve created our new model, let’s see the results. We’ll repeat the steps we took before to find the p-values.\n\n\nCode\n# Extract the summaries for the model\nlabor_glm_summary &lt;- summary(labor_glm)$coefficients\n\n# Convert to data frames for easier manipulation\nlabor_glm_df &lt;- as.data.frame(labor_glm_summary)\n\n# Populate the data frame with the variable names and their corresponding values\ncolnames(labor_glm_df) &lt;- c(\"Estimate\", \"Std. Error\", \"t value\", \"p-value\")\nlabor_glm_df$Variable &lt;- rownames(labor_glm_df)\nlabor_glm_df$Model &lt;- \"GLM Labor\"\n\n# Remove row names (index) by resetting the row names to NULL\nrownames(labor_glm_df) &lt;- NULL\n\nlabor_glm_table &lt;- kable(labor_glm_df[, c(\"Variable\", \"Estimate\", \"Std. Error\", \"t value\", \"p-value\")],\n                            digits = 3,\n                            caption = \"Regression Coefficients for Labor Generalized Linear Model\") %&gt;% \n kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"center\") %&gt;%\n  column_spec(1:5, background = \"white\")\n\nlabor_glm_table\n\n\n\nRegression Coefficients for Labor Generalized Linear Model\n\n\nVariable\nEstimate\nStd. Error\nt value\np-value\n\n\n\n\n(Intercept)\n0.807\n0.020\n41.041\n0.000\n\n\navg_pesticide\n0.069\n0.071\n0.964\n0.341\n\n\nyears_elapsed\n-0.022\n0.003\n-6.353\n0.000\n\n\n\n\n\n\n\nProgress! It looks like our p-value for ‘avg_pesticide’ is roughly half of what our last model produced. This is a meaningless improvement however, as the new value is still statistically insignificant at \\(\\alpha\\) &lt; 0.05 with a value of 0.34. Let’s check to see if this new method produced data that is normal and homoscedastic, using the same methods from before.\n\nLogistic Model Evaluation\n\n\nCode\n# Generate predictions and residuals for avg_total_labor using the glm model\ncombined_glm &lt;- combined_tidy %&gt;%\n  mutate(predicted_labor = predict(labor_glm, type = \"response\")) %&gt;% \n  mutate(residuals_glm = residuals.glm(labor_glm))\n\n\n# Plot a histogram of glm residuals to check normality\nglm_hist_plot &lt;- ggplot(combined_glm, aes(x = residuals_glm)) +\n  geom_histogram(bins = 30, fill = \"lavender\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Histogram of GLM Residuals\", x = \"Residuals\", y = \"Frequency\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n# Make a Q-Q plot to check normality\nglm_qq_plot &lt;- ggplot(combined_glm, aes(sample = predicted_labor)) +\n  stat_qq() +\n  stat_qq_line(color = \"firebrick\", alpha = 0.5) +\n  labs(title = \"Q-Q Plot of GLM Residuals\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n# Create the plot\nglm_fitted_resdid_plot &lt;- ggplot(combined_glm, aes(x = predicted_labor, y = residuals_glm)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"tomato\", linetype = \"dashed\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"GLM Residuals vs Fitted Values\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n# Display the graphs\nglm_hist_plot\n\n\n\n\n\n\n\n\n\nCode\nglm_qq_plot\n\n\n\n\n\n\n\n\n\nCode\nglm_fitted_resdid_plot\n\n\n\n\n\n\n\n\n\nAfter further investigation, it’s clear that this model didn’t produce normally distributed data either. Our histogram of GLM residuals is closer to a normal distribution, but it’s asymmetric with a right tail. Our Q-Q plot of GLM residuals is also a better fit, with at least half of the points landing on the line of normality. Still, the extreme high and low values deviate from the line. When we plotted the fitted GLM data versus the residual GLM data, the result is a funnel similar to the first graph. Though improved, this model did not produce normal or homoscedastic data and is therefore not worthwhile as a means to predict total labor cost from pesticide use and year alone.\n\n\nVisualizing Both Linear and Non-Linear Models\nLet’s compare each model by plotting our data and regression lines. Can we see a demontrable difference in outcome?\n\n\nCode\n# Create the plot for Total Labor Model\ntotal_labor_plot &lt;- ggplot(combined_glm, aes(x = years_elapsed, y = avg_total_labor)) +\n  geom_point(color = \"slategray\", size = 2) +  # Scatter plot for avg_total_labor\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"firebrick\", linetype = \"solid\") +  # Regression line for total_labor_model\n  labs(subtitle = \"Linear Model\", title = \"Total Labor Demand After 1960\", x = \"Years Past 1960\", y = \"Average Total Labor\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5), # Center the title\n        plot.subtitle = element_text(hjust = 0.5))  # Center the subtitle\n\n\n# Create the plot for Total Labor GLM\ntotal_labor_glm_plot &lt;- ggplot(combined_glm, aes(x = years_elapsed, y = predicted_labor)) +\n  geom_point(color = \"forestgreen\", size = 2) +  # Scatter plot for avg_total_labor\n  geom_line(aes(y = predicted_labor),, color = \"slateblue\", linetype = \"solid\") +  # Regression line for total_labor_model\n  labs(subtitle = \"Generalized Linear Model\", title = \"Predicted Total Labor Demand \\nAfter 1960\", x = \"Years Past 1960\", y = \"Predicted Average Total Labor\") +\n  theme_minimal()  +\n  theme(plot.title = element_text(hjust = 0.5), # Center the subtitle\n        plot.subtitle = element_text(hjust = 0.5))  # Center the title\n\n# Display both plots side by side\ntotal_labor_plot + total_labor_glm_plot + plot_layout(ncol = 2)"
  },
  {
    "objectID": "safe.html#results",
    "href": "safe.html#results",
    "title": "Examing the Impact of Fertilizer & Pesticide Use on Agricultural Labor Demands",
    "section": "Results",
    "text": "Results\nThrough our correlation analysis, we found that average national agricultural pesticide and fertilizer use has exploded since 1960. These two variables are closely correlated, with \\(r\\) values of 0.82. This suggests that most growers who use fertilizers will also rely on pesticides in their approach to farming.\nOur linear regression analysis also found that labor demand has dropped significantly in the same time, less than half what it once was. The resulting negative correlation between labor demand and synthetic inputs is not so clear cut, however. When looking at any other predictor variable, years elapsed from 1960 had extremely strong correlation, positive or negative.\nFurther investigation of the efficacy of our model suggest that it is a poor fit to draw any conclusions from. When plotted, the data did not have a normal distribution, the variance was not consistent, diverging at extreme values, and the coefficient p-values were not of significance.\nA revised logistic regression had slightly more convincing results, but still nothing concrete. We were able to get our coefficient p-value to half of what it was initially, and the data was more normally distributed, but it remained heteroscedastic and the p-value was still insgnificant despite being lower.\nLooking back at our Null and Alternative hypotheses, it’s impossible to be conclusive. Based on our findings, we fail to reject our null hypothesis that pesticide and fertilizer use do not have a significant effect on labor demand. Both of our predictive models yielded coefficient p-values notably higher than the convention for significance of \\(alpha\\) &lt; 0.05.\nOur results suggest that there are other factors not included in our models that have a significant impact on agricultural labor demand. This is known as Omitted variable bias. Omitted variable bias occurs when a relevant predictive variable is excluded from a model, resulting in biased estimates for the included variables’ coefficients. This happens because the excluded variable is correlated with both the dependent variable and one or more of the independent variables, leading to inaccurate projections.\nDespite our findings that neither model is a perfect fit, we can still learn something from them. Visualized, we can see that our GLM on the right does provide a better fit for our scenario. This means that labor demand is not a fixed linear variable, it decays over time. Though not dramatic, our predictive line does curve slightly. This suggests that labor demand is influenced by factors that affect it’s trend of decay as well as the demand itself.\n\nAdditional Considerations & Possible Next Steps\nTo improve on this analysis in the future, omitted variables need to be identified and included in the logistic regression model. There are likely more than one significant predictors that are closely correlated to our ‘years_elapsed’ variable.\n\n\n\nextension.umn.edu\n\n\nThese might include the introduction of cost-saving technologies, higher yielding and pest or disease resistant crops, higher domestic wages, cost of living changes, inflation, an increase in availability of cheaper migrant labor, broader economic trends, or other unidentified predictors. It is possible that one or more of these factors has relevance to the model, and including them w would further mitigate omitted variable bias and improve the interpretability and accuracy of the model.\nFurthermore, more localized, targeted data might be helpful in understanding trends. Comparing data from different regions or states and including relevant economic factors could aid in evaluating the connection between synthetic inputs and labor if there is one. A comparison between conventional and organic farms would also be valuable, though the relatively small number and size of the latter would be an obstacle to overcome.\n\n\nReferences\n1: U.S. Department of Agriculture, Economic Research Service. (2024). Agricultural productivity in the United States. https://www.ers.usda.gov/data-products/agricultural-productivity-in-the-united-states/. Accessed: November 11, 2024.\n2: U.S. Department of Agriculture, Economic Research Service. (2024). Fertilizer & Pesticide Use in the United States https://www.ers.usda.gov/webdocs/DataFiles/47679/table17.xls?v=0. Accessed: November 11, 2024.\n\n\nData Availability\nThe report Fertilizer & Pesticide Use in the United States is no longer available online at the source above. The data can be found in the repository for this post.\n\n\nAcknowledgements\nThis post is based on a project from Max Czapanskiy for EDS-222: Statistics for Environmental Data Scientists."
  }
]